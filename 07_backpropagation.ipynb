{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd89bdbb",
   "metadata": {},
   "source": [
    "### 미분\n",
    "- 미분의 기본 개념은 함수의 변화율을 구하는 데 있다. 이는 함수가 입력의 변화에 따라 얼마나 변하는지를 수학적으로 분석하는 도구이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e03663",
   "metadata": {},
   "source": [
    "### 01-01. 평균 변화율\n",
    "\n",
    "먼저, 함수 $f(x)$의 **평균 변화율**을 살펴본다. 이는 두 점 $x$와 $x + h$ 사이에서 함수 $f(x)$가 얼마나 변했는지를 나타내는 지표이다.\n",
    "\n",
    "$$\n",
    "평균변화율 = \\frac{f(x + h) − f(x)}{h}\n",
    "$$\n",
    "\n",
    "예를 들어, $f(x) = 2x$라 할 때, $x=5$에서 $x=8$로 바뀌었다고 하면, 평균 변화율은 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\frac{f(8) - f(5)}{3} = \\frac{16 - 10}{3} = 2\n",
    "$$\n",
    "\n",
    "이는 $h$만큼의 변화에 대해 함수 $f$가 얼마나 변하는지를 나타내며, 기울기의 개념과도 연결된다.\n",
    "\n",
    "### 01-02. 순간 변화율 (미분의 정의)\n",
    "\n",
    "**순간 변화율**은 평균 변화율에서 $h$가 0에 극한으로 가까워질 때를 의미한다. 이를 통해 미분을 정의할 수 있다.\n",
    "\n",
    "$$\n",
    "f′(x) = \\lim_{h→0}\\frac{f(x + h) − f(x)}{h}\n",
    "$$\n",
    "\n",
    "**예: $f(x) = x^2$의 미분**\n",
    "\n",
    "$$\n",
    "\\frac{(x + h)^2 − x^2}{h} = \\frac{x^2 + 2xh + h^2 − x^2}{h} = \\frac{2xh + h^2}{h} = 2x + h\n",
    "$$\n",
    "\n",
    "극한을 취하면 다음과 같다.\n",
    "\n",
    "$$\n",
    "\\lim_{h → 0}(2x + h) = 2x\n",
    "$$\n",
    "\n",
    "따라서, $f(x) = x^2$의 미분은 $f′(x) = 2x$이다.\n",
    "\n",
    "### 01-03. 다양한 미분 예시\n",
    "\n",
    "#### 01-03-01. 다항 함수 $f(x) = x^2$\n",
    "\n",
    "위와 같이 전개 및 극한을 통해 다음을 얻는다.\n",
    "\n",
    "$$\n",
    "f′(x) = 2x\n",
    "$$\n",
    "\n",
    "#### 01-03-02. 상수 함수 $f(x) = c$\n",
    "\n",
    "$$\n",
    "\\frac{f(x + h) − f(x)}{h} = \\frac{c − c}{h} = 0\n",
    "$$\n",
    "\n",
    "상수 함수는 입력 변화에 관계없이 값이 일정하므로 미분값은 항상 0이다.\n",
    "\n",
    "$$\n",
    "f′(x) = 0\n",
    "$$\n",
    "\n",
    "#### 01-03-03. 선형 함수 $f(x) = mx + b$\n",
    "\n",
    "$$\n",
    "\\frac{f(x + h) − f(x)}{h} = \\frac{m(x + h) + b − (mx + b)}{h} = \\frac{mh}{h} = m\n",
    "$$\n",
    "\n",
    "따라서, 선형 함수의 미분은 $f′(x) = m$이다.\n",
    "\n",
    "### 01-04. 미분의 기본 규칙\n",
    "\n",
    "1. 상수 $c$의 미분은 $0$이다.\n",
    "2. $f(x) = x^n$ 형태의 미분은 $nx^{n−1}$이다.\n",
    "3. 함수의 선형 조합 $af(x) + bg(x)$의 미분은 $af′(x) + bg′(x)$이다.\n",
    "\n",
    "### 01-05. 미분 표기\n",
    "\n",
    "라이프니츠는 미분을 다음과 같은 표기로 정의하였다:\n",
    "\n",
    "* $d$: differential, 미소한 변화\n",
    "* $dx$: $x$의 미소 변화\n",
    "* $\\frac{d}{dx}$: $x$에 대한 미분 연산자\n",
    "\n",
    "**연쇄 법칙 표기**\n",
    "\n",
    "합성 함수 $z = f(g(x))$에 대해 다음과 같이 연쇄법칙을 적용할 수 있다:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "### 01-06. 미분의 연쇄 법칙\n",
    "\n",
    "합성 함수 $y = f(g(x))$의 미분은 내부 함수와 외부 함수의 미분을 곱하여 구한다.\n",
    "\n",
    "* $y = f(u)$\n",
    "* $u = g(x)$\n",
    "\n",
    "이때:\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx}\n",
    "$$\n",
    "\n",
    "**예시**\n",
    "\n",
    "$z = y^2$, $y = 5x + 2$인 경우,\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} = 2y \\cdot 5 = 10y\n",
    "$$\n",
    "\n",
    "$y = 5x + 2$이므로:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = 10(5x + 2) = 50x + 20\n",
    "$$\n",
    "\n",
    "이는 직접 미분한 결과와 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf099e8",
   "metadata": {},
   "source": [
    "### 오차 역전파 (Backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b128383",
   "metadata": {},
   "source": [
    "![](https://d.pr/i/4nYlr4+)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab86e708",
   "metadata": {},
   "source": [
    "**오차역전파의 단계**\n",
    "1. **순전파(Forward Propagation)**: 입력 데이터가 네트워크를 통과하며 예측값을 생성한다.\n",
    "2. **오차 계산(Error Calculation)**: 예측값과 실제 목표값 사이의 오차를 계산한다. 대표적인 오차 함수는 평균 제곱 오차(MSE)이다.\n",
    "3. **오차 역전파(Backpropagation)**: 오차를 네트워크의 각 가중치로 전파하여 가중치를 조정한다.\n",
    "4. **가중치 갱신(Update Weights)**: 경사하강법을 통해 오차가 감소하도록 각 가중치를 갱신한다.\n",
    "\n",
    "\n",
    "**간단히 단일 계층(입력 $ x $ -> 은닉층 $ h $ -> 출력 $ y $) 신경망에서 오차역전파 설명**\n",
    "\n",
    "![](https://d.pr/i/juFjHc+)\n",
    "\n",
    "1. **순전파 단계**\n",
    "\n",
    "    - 가중치 $ w_1 $와 $ w_2 $가 각각 입력과 은닉층, 은닉층과 출력층 사이에 존재한다고 하자.\n",
    "    - 입력 $ x $가 은닉층 $ h $에 도달하면서 가중치 $ w_1 $을 곱한 뒤 활성화 함수를 적용:\n",
    "      $\n",
    "      h = f(x \\cdot w_1)\n",
    "      $\n",
    "    - 은닉층 출력 $ h $가 출력층 $ y $로 전달되며 가중치 $ w_2 $를 곱한 뒤 활성화 함수를 적용하여 최종 출력:\n",
    "      $\n",
    "      y = f(h \\cdot w_2)\n",
    "      $\n",
    "\n",
    "2. **오차 계산**\n",
    "\n",
    "    - 예측된 출력 $ y $와 목표 값 $ t $ 사이의 오차 $ E $를 계산한다. 여기서 평균 제곱 오차(MSE)를 사용하면:\n",
    "      $\n",
    "      E = \\frac{1}{2}(t - y)^2\n",
    "      $\n",
    "\n",
    "3. **오차의 기울기 계산**\n",
    "\n",
    "    - 오차 $ E $를 최소화하기 위해 각 가중치 $ w_1 $과 $ w_2 $에 대한 편미분을 구해야 한다.\n",
    "    - 출력층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_2}\n",
    "      $\n",
    "    - 은닉층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w_1}\n",
    "      $\n",
    "\n",
    "4. **가중치 갱신**\n",
    "\n",
    "    - 각 가중치는 학습률 $ \\eta $와 오차 기울기를 사용해 갱신한다:\n",
    "      $\n",
    "      w_2 = w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2}\n",
    "      $\n",
    "      $\n",
    "      w_1 = w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1}\n",
    "      $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5efd704",
   "metadata": {},
   "source": [
    "![](https://d.pr/i/4nYlr4ㅁ+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = (y(h(x, w1, w2)) - y)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64d1cd",
   "metadata": {},
   "source": [
    "## 손실함수/활성화함수의 도함수\n",
    "\n",
    "\n",
    "### 손실 함수 (Loss Functions)의 도함수\n",
    "\n",
    "| 손실 함수 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **MSELoss** (Mean Squared Error) | $L = \\frac{1}{n} \\sum (y - \\hat{y})^2$ | $\\frac{dL}{d\\hat{y}} = \\frac{2}{n} (\\hat{y} - y)$ | 예측값과 정답의 차이를 제곱해 평균, **이상치에 민감함** |\n",
    "| **L1Loss** (Mean Absolute Error) | $L = \\frac{1}{n} \\sum \\|y - \\hat{y}\\|$ | $\\frac{dL}{d\\hat{y}} = \\frac{1}{n} \\cdot \\text{sign}(\\hat{y} - y)$ | 절댓값 오차 평균, **이상치에 덜 민감**하지만 미분 불연속 |\n",
    "| **HuberLoss** | $L = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } \\|y - \\hat{y}\\| \\leq \\delta \\\\ \\delta \\cdot (\\|y - \\hat{y}\\| - \\frac{1}{2} \\delta) & \\text{otherwise} \\end{cases}$ | $\\frac{dL}{d\\hat{y}} = \\begin{cases} \\hat{y} - y & \\text{if } \\|\\hat{y} - y\\| \\leq \\delta \\\\ \\delta \\cdot \\text{sign}(\\hat{y} - y) & \\text{otherwise} \\end{cases}$ | MSE와 MAE의 장점 결합, **이상치에 덜 민감하면서 부드러운 미분** |\n",
    "| **BCELoss** (Binary Cross Entropy) | $L = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)$ | $\\frac{dL}{d\\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)$ | **이진 분류**에서 사용, 출력에 **sigmoid**를 적용해야 함 |\n",
    "| **BCEWithLogitsLoss** | $L = \\max(z, 0) - z \\cdot y + \\log(1 + e^{-\\|z\\|})$ | $\\frac{dL}{dz} = \\sigma(z) - y$ | sigmoid + BCELoss 결합 형태, **수치적으로 안정적** |\n",
    "| **CrossEntropyLoss** | $L = - \\log \\left( \\frac{e^{z_y}}{\\sum_j e^{z_j}} \\right )$ | $\\frac{dL}{dz_i} = \\text{softmax}(z_i) - y_i$ | 다중 클래스 분류용, 내부에서 softmax 포함, **출력에 softmax 불필요** |\n",
    "\n",
    "\n",
    "\n",
    "### 활성화 함수 (Activation Functions)의 도함수\n",
    "\n",
    "\n",
    "| 함수 이름 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ | $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ | 이진 분류 확률 출력에 주로 사용 |\n",
    "| **Softmax** | $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $\\frac{\\partial s_i}{\\partial x_j} = s_i (\\delta_{ij} - s_j)$ | 다중 클래스 확률 출력, 출력 간 상호작용 있음 |\n",
    "| **ReLU** | $ReLU(x) = \\max(0, x)$ | $ReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$ | 계산 간단하고 많이 사용됨 |\n",
    "| **LeakyReLU** | $LeakyReLU(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{otherwise} \\end{cases}$ | $LeakyReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{otherwise} \\end{cases}$ | 음수 영역도 일부 통과시켜 죽은 ReLU 문제 완화 |\n",
    "| **Tanh** | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $\\tanh'(x) = 1 - \\tanh^2(x)$ | 출력이 -1~1 범위로 중심 정규화 효과 |\n",
    "| **Softmax + CrossEntropy** | $L = -\\sum_i y_i \\log(s_i)$ | $\\frac{\\partial L}{\\partial x_i} = s_i - y_i$ | softmax와 cross-entropy를 결합한 경우, 도함수가 매우 간단해짐 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e78b0b",
   "metadata": {},
   "source": [
    "### 연쇄 법칙\n",
    "\n",
    "- 기본 수식의 역전파 & 연쇄법칙 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0eae02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "552d343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파 결과: 9.0\n",
      "역전파 결과: 6.0\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    y = x ** 2\n",
    "    return y\n",
    "\n",
    "def backward(x):\n",
    "    dy_dx = 2 * x\n",
    "    return dy_dx\n",
    "\n",
    "x = 3.0\n",
    "print('순전파 결과:', forward(x))\n",
    "print('역전파 결과:', backward(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward(x, w1, w2):\n",
    "    h = f(x * w1)\n",
    "    y = f(h * w2)\n",
    "    return y\n",
    "\n",
    "\n",
    "def backward(x, w1, w2, y, t):\n",
    "    # 오차 계산\n",
    "    E = (y - t)**2\n",
    "    # 오차 기울기 계산\n",
    "    dE_dy = 2*(y - t)\n",
    "    # 출력층 가중치 기울기 계산\n",
    "    dE_dw2 = dE_dy * y * (1 - y)\n",
    "    # 은닉층 가중치 기울기 계산\n",
    "    dE_dh = dE_dy * w2 * (1 - h**2)\n",
    "    # 입력층 가중치 기울기 계산\n",
    "    dE_dw1 = dE_dh * x\n",
    "    # 가중치 갱신\n",
    "    w1 -= learning_rate * dE_dw1\n",
    "    w2 -= learning_rate * dE_dw2\n",
    "    return w1, w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76118c5e",
   "metadata": {},
   "source": [
    "- 다층 신경망에서 연쇄법칙 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74e2882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파 결과: 18.0\n",
      "역전파 결과: 12.0\n"
     ]
    }
   ],
   "source": [
    "def forward(x):\n",
    "    y = x ** 2\n",
    "    z = 2 * y\n",
    "    return z\n",
    "\n",
    "def backward(x):\n",
    "    dy_dx = 2 * x\n",
    "    dz_dy = 2\n",
    "    dz_dx = dz_dy * dy_dx\n",
    "    return dz_dx\n",
    "\n",
    "x = 3.0\n",
    "print('순전파 결과:', forward(x))\n",
    "print('역전파 결과:', backward(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023079f",
   "metadata": {},
   "source": [
    "### 신경망에서의 활용\n",
    "\n",
    " - 단순 신경망 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00884895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "X = np.array([0.5, 0.8])\n",
    "y = np.array([1.0])\n",
    "\n",
    "W = np.array([0.2, 0.4])\n",
    "\n",
    "# 순전파\n",
    "z = np.dot(X, W)\n",
    "r = sigmoid(z)\n",
    "\n",
    "mean = 0.5\n",
    "# 오차 계산\n",
    "loss = mean * (y - r)**2     # 오차 함수, 평균 제곱 오차, 평균은 임의로 0.5로 설정(임의로 설정해도 상관 없음)\n",
    "\n",
    "# 역전파 (기울기 계산)\n",
    "delta = (r - y) * sigmoid_derivative(z)\n",
    "grad_w = delta * X\n",
    "\n",
    "learning_rate = 0.1\n",
    "# 가중치 갱신\n",
    "W -= learning_rate * grad_w\n",
    "\n",
    "print(f\"오차: {loss}\")\n",
    "print(f\"가중치: {W}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eee3c08",
   "metadata": {},
   "source": [
    "- 은닉층 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b67d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20044   0.400528 ]\n",
      " [0.100704  0.3008448]]\n",
      "[[0.5004928]\n",
      " [0.6011264]]\n"
     ]
    }
   ],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "X = np.array([0.5, 0.8])                     # shape: (2,)\n",
    "y = np.array([1.0])                          # shape: (1,)\n",
    "\n",
    "W1 = np.array([[0.2, 0.4], [0.1, 0.3]])      # shape: (2, 2) -> X features 2개를 2개의 은닉층으로 매핑\n",
    "b1 = np.array([0.1, 0.2])                    # shape: (2,)\n",
    "W2 = np.array([[0.5], [0.6]])                # shape: (2, 1) -> 2개의 입력을 1개의 출력으로 매핑\n",
    "b2 = np.array([0.3])                         # shape: (1,)\n",
    "\n",
    "# 순전파\n",
    "z1 = np.dot(X, W1) + b1\n",
    "r1 = relu(z1)\n",
    "\n",
    "z2 = np.dot(r1, W2) + b2\n",
    "r2 = relu(z2)\n",
    "\n",
    "# 손실 계산 -> 역전파 (기울기 계산)\n",
    "delta2 = (r2 - y) * relu_derivative(z2)\n",
    "grad_W2 = np.outer(r1, delta2)\n",
    "\n",
    "delta1 = np.dot(W2, delta2) * relu_derivative(z1)\n",
    "grad_W1 = np.outer(X, delta1)\n",
    "\n",
    "# 가중치 갱신\n",
    "learning_rate = 0.01\n",
    "W2 -= learning_rate * grad_W2\n",
    "W1 -= learning_rate * grad_W1\n",
    "\n",
    "print(W1)\n",
    "print(W2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453b37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4  5]\n",
      " [ 8 10]\n",
      " [12 15]]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3])\n",
    "arr2 = np.array([4, 5])\n",
    "\n",
    "print(np.outer(arr1, arr2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a8d12",
   "metadata": {},
   "source": [
    "### 수치미분과 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35eee7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.000000000039306\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "def num_d_gradient(f, x):\n",
    "    h = 1e-5\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "def back_gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "print(num_d_gradient(f, 3.0))\n",
    "print(back_gradient(3.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7f45c",
   "metadata": {},
   "source": [
    "### 숫자 맞추기 AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32374947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100: 예측값 = 8.428829836377318, 손실 = 10.659930958470673\n",
      "Epoch 200: 예측값 = 11.326803882650859, 손실 = 1.4282140838242223\n",
      "Epoch 300: 예측값 = 12.387556107757415, 손실 = 0.1913516585783316\n",
      "Epoch 400: 예측값 = 12.775825728223953, 손실 = 0.025637232999857398\n",
      "Epoch 500: 예측값 = 12.9179449664486, 손실 = 0.0034348681415781114\n",
      "타겟 넘버: 13\n",
      "최종 예측값: 12.9179449664486\n"
     ]
    }
   ],
   "source": [
    "target_number = 13\n",
    "\n",
    "guess = np.random.rand()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 오차 계산\n",
    "    loss = 0.5 * ((guess - target_number) ** 2)\n",
    "\n",
    "    # 역전파 (기울기 계산)\n",
    "    grad = guess - target_number\n",
    "\n",
    "    # 업데이트 (guess 업데이트)\n",
    "    guess -= learning_rate * grad\n",
    "\n",
    "    # epoch 100마다 예측값과 손실 출력\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}: 예측값 = {guess}, 손실 = {loss}\")\n",
    "\n",
    "# 최종 예측값 guess 출력\n",
    "print(f\"타겟 넘버: {target_number}\")\n",
    "print(f\"최종 예측값: {guess}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
