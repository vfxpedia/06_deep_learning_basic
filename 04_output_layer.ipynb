{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1701630f",
   "metadata": {},
   "source": [
    "# 출력층 설계 (Output Layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b9a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n",
      "1.0\n",
      "[0.09003057 0.24472847 0.66524096]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "x1 = np.array([1000, 1001, 1002]) # [nan, nan, nan] 왜 이런 결과가 나오나? \n",
    "# overflow 발생 : 지수 함수의 결과가 너무 크게 나옴\n",
    "\n",
    "def stable_softmax(z): # 안정화된 softmax 함수, 예시에서는 1002가 제일 큰 수이므로 1002를 빼줌, np.array([-2, -1, 0]) → 오버플로우 발생하지 않음\n",
    "    exp_z = np.exp(z - np.max(z))\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "x = np.array([10, 11, 12])\n",
    "print(softmax(x))\n",
    "print(sum(softmax(x)))\n",
    "print(stable_softmax(x1))       # 오버플로우 발생하지 않음, softmax는 비율을 계산하는 것이므로 max(z)값을 똑같이 빼줘도 비율은 같음\n",
    "print(sum(stable_softmax(x1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b044a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_20040\\3184034056.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(1000) # → 계산할 수 없는 범위가 된다. → 오버플로우 발생\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(inf)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1000) # → 계산할 수 없는 범위가 된다. → 오버플로우 발생\n",
    "# overflow encountered in exp  np.exp(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78144c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_pred, y_true):\n",
    "    return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha * z, z)\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n",
    "def elu(z, alpha=1.0):\n",
    "    return np.where(z > 0, z, alpha * (np.exp(z) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e7d5cb",
   "metadata": {},
   "source": [
    "- pytorch 라이브러리 함수 사용\n",
    "\n",
    "    딥러닝 프레임워크 → 딥러닝에 사용될 여러 함수를 제공하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a33df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0900, 0.2447, 0.6652])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1000, 1001, 1002], dtype=torch.float32)\n",
    "softmax = nn.Softmax(dim=0)\n",
    "print(softmax(x))\n",
    "F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133001e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1000, 1001, 1002], dtype=torch.float32)\n",
    "\n",
    "softmax_output = F.softmax(x, dim=0)    # 결과값이 stable_softmax 함수와 같음 \n",
    "                                        # → 내부적으로 안정화된 softmax 함수를 사용하고 있음 → Overflow 발생하지 않음\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d233d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m x = np.array([\u001b[32m1000\u001b[39m, \u001b[32m1001\u001b[39m, \u001b[32m1002\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m softmax_output = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(softmax_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\functional.py:2137\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2135\u001b[39m     dim = _get_softmax_dim(\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m.dim(), _stacklevel)\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2137\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m(dim)\n\u001b[32m   2138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2139\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.softmax(dim, dtype=dtype)\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "x = np.array([1000, 1001, 1002])    → 반드시 tensor 형태로 변환해서 입력값을 만들어줘야한다.\n",
    "\n",
    "softmax_output = F.softmax(x, dim=0)\n",
    "print(softmax_output)\n",
    "\n",
    "'numpy.ndarray' object has no attribute 'softmax' → numpy 배열에는 softmax 함수가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f75dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "\"softmax_lastdim_kernel_impl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m x = torch.tensor([\u001b[32m1000\u001b[39m, \u001b[32m1001\u001b[39m, \u001b[32m1002\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m softmax_output = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m    \u001b[38;5;66;03m# 결과값이 stable_softmax 함수와 같음 \u001b[39;00m\n\u001b[32m      4\u001b[39m                                         \u001b[38;5;66;03m# → 내부적으로 안정화된 softmax 함수를 사용하고 있음 → Overflow 발생하지 않음\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(softmax_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\functional.py:2137\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2135\u001b[39m     dim = _get_softmax_dim(\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m.dim(), _stacklevel)\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2137\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2139\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.softmax(dim, dtype=dtype)\n",
      "\u001b[31mNotImplementedError\u001b[39m: \"softmax_lastdim_kernel_impl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1000, 1001, 1002])    → \"softmax_lastdim_kernel_impl\" not implemented for 'Long'\n",
    "# 반드시 float32 형태로 변환해서 입력값을 만들어줘야한다. 데이터의 타입을 맞춰줘야한다.\n",
    "# 우리는 softmax 함수를 사용할 것이기 때문에 long (int64) 타입이 아닌 float32 타입으로 변환해야한다.\n",
    "\n",
    "softmax_output = F.softmax(x, dim=0)    # 결과값이 stable_softmax 함수와 같음 \n",
    "                                        # → 내부적으로 안정화된 softmax 함수를 사용하고 있음 → Overflow 발생하지 않음\n",
    "print(softmax_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fd91b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m x = torch.tensor([\u001b[32m1000\u001b[39m, \u001b[32m1001\u001b[39m, \u001b[32m1002\u001b[39m], dtype=torch.float32)\n\u001b[32m      5\u001b[39m softmax = nn.Softmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m      7\u001b[39m F.softmax(x, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1679\u001b[39m, in \u001b[36mSoftmax.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m   1678\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1679\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Playdata\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\functional.py:2137\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   2135\u001b[39m     dim = _get_softmax_dim(\u001b[33m\"\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m.dim(), _stacklevel)\n\u001b[32m   2136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2137\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2139\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.softmax(dim, dtype=dtype)\n",
      "\u001b[31mIndexError\u001b[39m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1000, 1001, 1002], dtype=torch.float32)\n",
    "softmax = nn.Softmax(dim=1) → dim 값이 1이면 차원 범위 오류 발생, dim 값은 0으로 설정정\n",
    "Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "print(softmax(x))\n",
    "F.softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed5df94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "x = torch.tensor([1000, 1001, 1002], dtype=torch.float32)\n",
    "\n",
    "sigmoid_output = torch.sigmoid(x)\n",
    "print(sigmoid_output)\n",
    "print(F.sigmoid(x))\n",
    "\n",
    "# → 결과값이 같음, 입력값이 모두 양수이므로 결과값이 모두 0.5 이상이 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905760d6",
   "metadata": {},
   "source": [
    "### 손실 함수와 연계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29c5903c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "머신러닝에서는 sklearn 라이브러리를 사용하여 모델을 정의하고 학습을 진행했음\n",
    "BaseEstimator 클래스를 상속받아 모델을 정의하고 학습을 진행했음.\n",
    "그리고 fit, predict, score 메서드를 사용하여 모델을 학습하고 예측하고 평가를 진행했음.\n",
    "\n",
    "딥러닝에서는 nn.Module 클래스를 상속받아 모델을 정의하고 학습을 진행.\n",
    "forward 메서드를 사용하여 모델을 정의하고 학습을 진행.\n",
    "backward 메서드를 사용하여 손실 함수를 계산하고 역전파를 진행.\n",
    "\n",
    "손실 함수를 사용하여 모델을 학습하고 예측하고 평가를 진행. → 손실값을 갖고 모델을 학습하고 예측하고 평가를 진행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e99e807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 다중 클래스 분류 모델 정의\n",
    "class SimpleMultiClassModel(nn.Module):\n",
    "    def __init__(self):                                            # → 모델의 구조를 정의하는 함수\n",
    "        super(SimpleMultiClassModel, self).__init__()\n",
    "        self.fc = nn.Linear(5, 3)    # fc : fully connected layer, 5개의 입력값을 3개의 출력값으로 변환\n",
    "        # 5개의 입력값 : 4개의 특성 + 1개의 바이어스\n",
    "        # 3개의 출력값 : 3개의 클래스\n",
    "        # 입력층에서 출력층으로 가는 선형 변환 과정, 모두 연결되어 있음 → 모든 입력값이 출력값에 영향을 줌\n",
    "        # nn.Linear() : 선형 변환 과정을 정의하는 함수 → 이 클래스는 선형 계산을 해주는 층이 존재하는 클래스\n",
    "\n",
    "    # 전체 과정을 정의하는 함수\n",
    "    def forward(self, x):                                          # → 순전파 과정을 정의하는 함수\n",
    "        return self.fc(x)\n",
    "\n",
    "    # # 손실 함수를 정의하는 함수                                    # → 본래는 손실함수를 만들어주는 것이 정석이나 지금 예제에서는 빼고 진행행\n",
    "    # def loss_fn(self, y_pred, y_true):\n",
    "    #     return F.cross_entropy(y_pred, y_true)\n",
    "\n",
    "model = SimpleMultiClassModel()                                    # 우리가 만든 SimpleMultiClassModel 클래스를 사용하여 모델을 정의\n",
    "criterion = nn.CrossEntropyLoss()                                  # → 손실 함수를 정의하는 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)                # → 최적화 Adam 알고리즘 최적화 함수를 사용\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# model.parameters() : 모델의 모든 파라미터를 가져오는 함수, → 가중치와 바이어스를 가져옴\n",
    "# lr : 학습률, → 학습률은 0.01로 설정, 경사하강법을 통해서 가중치와 바이어스를 업데이트 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11d610d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8790372610092163\n",
      "0.8682606220245361\n",
      "0.8504062294960022\n",
      "0.8268001079559326\n",
      "0.7986413240432739\n",
      "0.7670071125030518\n",
      "0.7328615188598633\n",
      "0.6970622539520264\n",
      "0.6603679060935974\n",
      "0.6234432458877563\n"
     ]
    }
   ],
   "source": [
    "# 데이터 생성\n",
    "inputs = torch.randn(4, 5)                                        # Sameple 4개의 데이터, 5개의 특성\n",
    "labels = torch.tensor([0, 2, 1, 0])                               # Sample 4개의 label 데이터, 3개의 클래스\n",
    "\n",
    "# 모델 학습\n",
    "for _ in range(10):                          # 학습을 임의로 10번 반복\n",
    "    preds = model(inputs)                    # 순전파 과정    → 모델에 입력값을 넣어서 예측(출력)값을 얻음\n",
    "    loss = criterion(preds, labels)          # 손실 함수 계산 → 예측값과 실제값을 비교하여 손실값을 계산\n",
    "    print(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()                    # 기울기 초기화  → 기울기를 0으로 초기화, 이전 단계에서 계산된 기울기를 0으로 초기화\n",
    "    loss.backward()                          # 역전파 과정    → 손실 함수를 미분하여 기울기를 계산, 손실에 대한 역전파 수행 (파라미터에 대한 기울기 계산)\n",
    "    optimizer.step()                         # 가중치와 바이어스 업데이트 → 기울기를 사용하여 가중치와 바이어스를 업데이트\n",
    "                                             # 계산된 기울기를 사용하여 옵티마이저가 모델의 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd2973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "optimizer.zero_grad()\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, outputs)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
